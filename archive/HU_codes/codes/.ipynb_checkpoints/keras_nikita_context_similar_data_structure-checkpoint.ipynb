{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl\n",
      "Collecting imbalanced-learn (from imblearn)\n",
      "  Downloading imbalanced-learn-0.2.1.tar.gz (90kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 660kB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.17.0 in /anaconda/lib/python3.5/site-packages (from imbalanced-learn->imblearn)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /anaconda/lib/python3.5/site-packages (from imbalanced-learn->imblearn)\n",
      "Requirement already satisfied: scikit-learn>=0.17.1 in /anaconda/lib/python3.5/site-packages (from imbalanced-learn->imblearn)\n",
      "Building wheels for collected packages: imbalanced-learn\n",
      "  Running setup.py bdist_wheel for imbalanced-learn ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
      "\u001b[?25h  Stored in directory: /Users/Kozodoi/Library/Caches/pip/wheels/b8/20/bd/0b775f7e5d413ac72562b1a5126598bcb6e0eae10da659be9f\n",
      "Successfully built imbalanced-learn\n",
      "Installing collected packages: imbalanced-learn, imblearn\n",
      "Successfully installed imbalanced-learn-0.2.1 imblearn-0.0\n"
     ]
    }
   ],
   "source": [
    "# libraries\n",
    "#!pip install rpy2\n",
    "#!pip install pandas\n",
    "#!pip install keras\n",
    "#!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# libraries\n",
    "import rpy2.robjects as robjects\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.layers import Input, Dense, Embedding, concatenate, Flatten, Dropout, Reshape\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "path = \"/Users/Kozodoi/Documents/Competitions/DSG_2017/\"\n",
    "known   = pd.read_csv(path + \"data/data_known.csv\")\n",
    "unknown = pd.read_csv(path + \"data/data_unknown.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(540370, 24)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop songs with a certain session_position\n",
    "#known = known.query(\"song_session_position == 1\")\n",
    "known.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# balancing the ratio of songs per session position\n",
    "known_new = pd.DataFrame()\n",
    "for i in unknown.song_session_position.unique():\n",
    "    frac_now  = len(unknown.query(\"song_session_position == \" + str(i)))/len(unknown)\n",
    "    known_now = known.query(\"song_session_position == \" + str(i))\n",
    "    known_now = known_now.sample(n = min(int(frac_now*len(known)), len(known_now)))\n",
    "    known_new = known_new.append(known_now)\n",
    "known = known_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "# Create a placeholder for the IDs new in the test data\n",
    "newUsers   = list(unknown.user_id[~unknown.user_id.isin(known.user_id)])       + list(known.user_id.value_counts().keys()[known.user_id.value_counts()     == 1])\n",
    "newSongs   = list(unknown.media_id[~unknown.media_id.isin(known.media_id)])    + list(known.media_id.value_counts().keys()[known.media_id.value_counts()   == 1])[-1000:]\n",
    "newArtists = list(unknown.artist_id[~unknown.artist_id.isin(known.artist_id)]) + list(known.artist_id.value_counts().keys()[known.artist_id.value_counts() == 1])[-1000:]\n",
    "newContext = list(unknown.context_type[~unknown.context_type.isin(known.context_type)]) + list(known.context_type.value_counts().keys()[known.context_type.value_counts() == 1])[-1000:]\n",
    "\n",
    "# In particular, assign IDs from 0 to N for users and songs\n",
    "# Use enumerate() to create a list of the new and original IDs\n",
    "users   = list(enumerate([i for i in known.user_id.unique()   if i not in newUsers]))\n",
    "songs   = list(enumerate([i for i in known.media_id.unique()  if i not in newSongs]))\n",
    "artists = list(enumerate([i for i in known.artist_id.unique() if i not in newArtists]))\n",
    "context = list(enumerate([i for i in known.context_type.unique() if i not in newContext]))\n",
    "\n",
    "# Create a dictionary with old IDs to new IDs\n",
    "userid2idx    = {o:i for i,o in users}\n",
    "songid2idx    = {o:i for i,o in songs}\n",
    "artistid2idx  = {o:i for i,o in artists}\n",
    "contextid2idx = {o:i for i,o in context}\n",
    "\n",
    "# Update with new/rare entries\n",
    "userid2idx.update({o:(max(userid2idx.values())+1)       for o in newUsers})\n",
    "songid2idx.update({o:(max(songid2idx.values())+1)       for o in newSongs})\n",
    "artistid2idx.update({o:(max(artistid2idx.values())+1)   for o in newArtists})\n",
    "contextid2idx.update({o:(max(contextid2idx.values())+1) for o in newContext})\n",
    "\n",
    "# Create id variable with the new IDs (known)\n",
    "known['userIdx']    = known.user_id.apply(lambda x:       userid2idx[x])\n",
    "known['songIdx']    = known.media_id.apply(lambda x:      songid2idx[x])\n",
    "known['artistIdx']  = known.artist_id.apply(lambda x:     artistid2idx[x])\n",
    "known['contextIdx'] = known.context_type.apply(lambda x:  contextid2idx[x])\n",
    "\n",
    "# Create id variable with the new IDs (unknown)\n",
    "unknown['userIdx']    = unknown.user_id.apply(lambda x:      userid2idx[x])\n",
    "unknown['songIdx']    = unknown.media_id.apply(lambda x:     songid2idx[x])\n",
    "unknown['artistIdx']  = unknown.artist_id.apply(lambda x:    artistid2idx[x])\n",
    "unknown['contextIdx'] = unknown.context_type.apply(lambda x: contextid2idx[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(441670, 28)\n",
      "(98700, 28)\n"
     ]
    }
   ],
   "source": [
    "# partition train and test data\n",
    "#ts = known.groupby([\"userIdx\"]).tail(3)\n",
    "#tr = known.groupby([\"userIdx\"], group_keys=False).apply(lambda x: x[:-3])\n",
    "tr = known.sample(frac = 0.8)\n",
    "ts = known.drop(tr.index)\n",
    "\n",
    "# move songs that appear only in ts to tr\n",
    "strayObs = ts.songIdx.isin(tr.songIdx) & ts.userIdx.isin(tr.userIdx) & ts.artistIdx.isin(tr.artistIdx) & ts.contextIdx.isin(tr.contextIdx)\n",
    "tr = tr.append(ts[~strayObs])\n",
    "ts = ts[strayObs]\n",
    "\n",
    "# print data size\n",
    "print(tr.shape)\n",
    "print(ts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saving the data samples\n",
    "#tr.to_csv(path + \"data/tr.csv\", index = False)\n",
    "#ts.to_csv(path + \"data/ts.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. INITIALIZING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an input layer with one row of IDs\n",
    "user_in    = Input(shape = (1,), dtype = 'int64',   name = \"user_in\")\n",
    "song_in    = Input(shape = (1,), dtype = 'int64',   name = \"song_in\")\n",
    "artist_in  = Input(shape = (1,), dtype = 'int64',   name = \"artist_in\")\n",
    "context_in = Input(shape = (1,), dtype = 'int64',   name = \"context_in\")\n",
    "num_in     = Input(shape = (1,), dtype = 'float32', name = \"num_in\")\n",
    "\n",
    "# Reshaping numeric features\n",
    "n = Reshape((1,1))(num_in)\n",
    "\n",
    "# Create an embedding assigning k latent factors to each ID\n",
    "# These will be optimized\n",
    "# A regulariztaion is added to avoid very large weights\n",
    "n_users   = tr.userIdx.nunique()\n",
    "n_songs   = tr.songIdx.nunique()\n",
    "n_artists = tr.artistIdx.nunique()\n",
    "n_context = tr.contextIdx.nunique()\n",
    "\n",
    "# Embeddings creation\n",
    "u = Embedding(n_users,   100, input_length=1, embeddings_regularizer=l2(1e-5))(user_in)\n",
    "s = Embedding(n_songs,   100, input_length=1, embeddings_regularizer=l2(1e-5))(song_in)\n",
    "a = Embedding(n_artists, 100, input_length=1, embeddings_regularizer=l2(1e-5))(artist_in)\n",
    "c = Embedding(n_context, 100, input_length=1, embeddings_regularizer=l2(1e-5))(context_in)\n",
    "\n",
    "# Specify what to do with the layers\n",
    "x = concatenate([u, c])\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(64, activation='relu')(x) \n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1, activation = \"sigmoid\")(x)\n",
    "\n",
    "# Then we specify the model that we want to use\n",
    "model = Model([user_in, context_in], x) \n",
    "model.compile(optimizer=\"Adagrad\", loss=\"binary_crossentropy\", metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. FIRST STAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 441670 samples, validate on 98700 samples\n",
      "Epoch 1/10\n",
      "441670/441670 [==============================] - 11s - loss: 0.5884 - acc: 0.6916 - val_loss: 0.6564 - val_acc: 0.6229\n",
      "Epoch 2/10\n",
      "441670/441670 [==============================] - 11s - loss: 0.5422 - acc: 0.7312 - val_loss: 0.6421 - val_acc: 0.6524\n",
      "Epoch 3/10\n",
      "441670/441670 [==============================] - 10s - loss: 0.5329 - acc: 0.7367 - val_loss: 0.6175 - val_acc: 0.6774\n",
      "Epoch 4/10\n",
      "441670/441670 [==============================] - 10s - loss: 0.5270 - acc: 0.7398 - val_loss: 0.5905 - val_acc: 0.7096\n",
      "Epoch 5/10\n",
      "441670/441670 [==============================] - 10s - loss: 0.5233 - acc: 0.7417 - val_loss: 0.5643 - val_acc: 0.7241\n",
      "Epoch 6/10\n",
      "441670/441670 [==============================] - 10s - loss: 0.5197 - acc: 0.7440 - val_loss: 0.5464 - val_acc: 0.7284\n",
      "Epoch 7/10\n",
      "441670/441670 [==============================] - 10s - loss: 0.5170 - acc: 0.7454 - val_loss: 0.5458 - val_acc: 0.7280\n",
      "Epoch 8/10\n",
      "441670/441670 [==============================] - 10s - loss: 0.5152 - acc: 0.7467 - val_loss: 0.5448 - val_acc: 0.7286\n",
      "Epoch 9/10\n",
      "441670/441670 [==============================] - 10s - loss: 0.5131 - acc: 0.7475 - val_loss: 0.5475 - val_acc: 0.7282\n",
      "Epoch 10/10\n",
      "441670/441670 [==============================] - 10s - loss: 0.5115 - acc: 0.7477 - val_loss: 0.5461 - val_acc: 0.7285\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x166f8af98>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the estimations on training data\n",
    "model.fit([tr.userIdx, tr.contextIdx], tr.is_listened, \n",
    "validation_data = ([ts.userIdx, ts.contextIdx], ts.is_listened),\n",
    "batch_size = int(len(tr)/100), epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_index</th>\n",
       "      <th>is_listened</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>264062</th>\n",
       "      <td>7232530</td>\n",
       "      <td>0.940844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1878642</th>\n",
       "      <td>2587335</td>\n",
       "      <td>0.918421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320138</th>\n",
       "      <td>3190521</td>\n",
       "      <td>0.655646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295749</th>\n",
       "      <td>5042661</td>\n",
       "      <td>0.430323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1548725</th>\n",
       "      <td>5953424</td>\n",
       "      <td>0.393673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         row_index  is_listened\n",
       "264062     7232530     0.940844\n",
       "1878642    2587335     0.918421\n",
       "320138     3190521     0.655646\n",
       "295749     5042661     0.430323\n",
       "1548725    5953424     0.393673"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict validation data\n",
    "pred = pd.DataFrame()\n",
    "pred[\"row_index\"] = ts.row_index\n",
    "pred[\"is_listened\"] = model.predict([ts.userIdx, ts.contextIdx])\n",
    "pred.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78770207943261217"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# computing AUC\n",
    "metrics.roc_auc_score(ts.is_listened, pred.is_listened)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. SECOND STAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "540370/540370 [==============================] - 11s - loss: 0.5175 - acc: 0.7450    \n",
      "Epoch 2/10\n",
      "540370/540370 [==============================] - 11s - loss: 0.5148 - acc: 0.7458    \n",
      "Epoch 3/10\n",
      "540370/540370 [==============================] - 11s - loss: 0.5134 - acc: 0.7467    \n",
      "Epoch 4/10\n",
      "540370/540370 [==============================] - 11s - loss: 0.5121 - acc: 0.7470    \n",
      "Epoch 5/10\n",
      "540370/540370 [==============================] - 11s - loss: 0.5111 - acc: 0.7480    \n",
      "Epoch 6/10\n",
      "540370/540370 [==============================] - 11s - loss: 0.5102 - acc: 0.7484    \n",
      "Epoch 7/10\n",
      "540370/540370 [==============================] - 11s - loss: 0.5092 - acc: 0.7488    \n",
      "Epoch 8/10\n",
      "540370/540370 [==============================] - 11s - loss: 0.5083 - acc: 0.7491    \n",
      "Epoch 9/10\n",
      "540370/540370 [==============================] - 11s - loss: 0.5077 - acc: 0.7496    \n",
      "Epoch 10/10\n",
      "540370/540370 [==============================] - 11s - loss: 0.5068 - acc: 0.7496    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1261c7b38>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the estimations on full known data\n",
    "model.fit([known.userIdx, known.contextIdx], known.is_listened,\n",
    "batch_size = int(known.shape[0]/100), epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>is_listened</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2262</th>\n",
       "      <td>14561</td>\n",
       "      <td>0.978413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4519</th>\n",
       "      <td>6026</td>\n",
       "      <td>0.984326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8168</th>\n",
       "      <td>9627</td>\n",
       "      <td>0.974721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8441</th>\n",
       "      <td>6064</td>\n",
       "      <td>0.981627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13121</th>\n",
       "      <td>8065</td>\n",
       "      <td>0.316997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sample_id  is_listened\n",
       "2262       14561     0.978413\n",
       "4519        6026     0.984326\n",
       "8168        9627     0.974721\n",
       "8441        6064     0.981627\n",
       "13121       8065     0.316997"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict unknown data\n",
    "pred = pd.DataFrame()\n",
    "pred[\"sample_id\"] = unknown.sample_id\n",
    "pred[\"is_listened\"] = model.predict([unknown.userIdx, unknown.contextIdx])\n",
    "pred.to_csv(path + \"submissions/keras_similar_data.csv\", index = False)\n",
    "pred.head(5)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
