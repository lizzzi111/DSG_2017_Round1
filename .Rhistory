install.packages("foreach")
install.packages("doParallel")
library(doParallel)
library(foreach)
foreach(i = 1:10) %do% {
print(i)
}
foreach(i = 1:1000) %do% {
print(i)
}
detectCores()
cl <- makeCluster(detectCores() - 2)
cores <- detectCores() - 2
cl <- makeCluster(cores)
registerDoParallel(cl)
foreach(i = 1:10000) %do% {
print(i)
}
foreach(i = 1:10000) %dopar% {
print(i)
}
proc.time()
t <- proc.time()
foreach(i = 1:10000) %dopar% {
print(i)
}
proc.time() - t
t <- proc.time()
foreach(i = 1:10000) %do% {
print(i)
}
proc.time() - t
t <- proc.time()
foreach(i = 1:10000) %do% {
print(i)
}
x <- proc.time() - t
t <- proc.time()
foreach(i = 1:10000) %dopar% {
print(i)
}
x1 <- proc.time() - t
x
x1
cores <- detectCores() - 2
cl <- makeCluster(cores)
registerDoParallel(cl)
t <- proc.time()
foreach(i = 1:100000) %dopar% {
print(i)
}
xpar <- proc.time() - t
t <- proc.time()
foreach(i = 1:100000) %do% {
print(i)
}
x <- proc.time() - t
xpar
x
rm(list = ls())
library(doParallel)
library(foreach)
cores <- detectCores() - 2
cl <- makeCluster(cores)
registerDoParallel(cl)
t <- proc.time()
foreach(i = 1:100000) %dopar% {
print(i)
}
xpar <- proc.time() - t
t <- proc.time()
foreach(i = 1:100000) %do% {
print(i)
}
xdo <- proc.time() - t
t <- proc.time()
for (i in 1:100000) {
print(i)
}
x <- proc.time() - t
xpar
xdo
x
rm(list = ls())
###################################
#                                 #
#             SETTINGS            #
#                                 #
###################################
# clearing the memory
rm(list = ls())
# setting work directory
work.folder <- "/Users/Kozodoi/Documents/Competitions/DSG_2017"
setwd(work.folder)
# setting inner folders
code.folder <- "codes"
data.folder <- "data"
func.folder <- "functions"
subm.folder <- "submissions"
# loading libraries
if(require(pacman)==FALSE) install.packages("pacman")
library(pacman)
p_load(data.table, AUC, anytime, beepr, compiler)
# loading functions
source(file.path(code.folder, "code_0_helper_functions.R"))
###################################
#                                 #
#        1. DATA PREPARATION      #
#                                 #
###################################
# loading data
data.full <- read.csv2(file.path(data.folder, "data_flow.csv"), sep = ",", dec = ".", header = T)
api.data <- read.delim(file.path(data.folder, "api_final.txt"))
head(api.data)
api.data <- read.delim(file.path(data.folder, "api_final.txt"), sep = ",", dec = ".")
head(api.data)
summary(api.data)
summary(data.full$media_id)
###################################
#                                 #
#             SETTINGS            #
#                                 #
###################################
# clearing the memory
rm(list = ls())
# setting work directory
# work.folder <- "N:/DSG2017/DSG_2017/"
work.folder <- "/Users/Kozodoi/Documents/Competitions/DSG_2017"
setwd(work.folder)
# setting inner folders
code.folder <- "codes"
data.folder <- "data"
func.folder <- "functions"
subm.folder <- "submissions"
# loading libraries
if(require(pacman)==FALSE) install.packages("pacman")
library(pacman)
p_load(data.table, anytime)
# loading functions
source(file.path(code.folder, "code_0_helper_functions.R"))
###################################
#                                 #
#         DATA PREPARATION        #
#                                 #
###################################
########## 1. LOADING THE DATA
# loading training data
data.train <- fread(file.path(data.folder, "train.csv"), sep = ",", dec = ".", header = T)
# loading testing data
data.test <- fread(file.path(data.folder, "test.csv"), sep = ",", dec = ".", header = T)
# merging data sets
data.test$is_listened <- NA
data.train$sample_id  <- NA
data.test$dataset <- "unknown"
data.train$dataset <- "train"
data.full <- rbind(data.train, data.test)
setkey(data.full, user_id, media_id)
rm(list = c("data.test",  "data.train"))
########## 2. CONVERTING VARIABLES
# converting factors
temp <- c("sample_id", "genre_id", "media_id", "album_id", "user_id", "artist_id", "context_type", "platform_name", "platform_family")
data.full[, (temp) := lapply(.SD, factor), .SDcols = temp]
# converting timestamps
data.full[, release_date := as.Date(as.character(data.full$release_date), "%Y%m%d")]
data.full[, ts_listen := anytime(data.full$ts_listen, asUTC = T)]
########## 3. CREATING FEATURES ON FULL DATA
### add data from json file, info on song name, album name, artist name
#if(file.exists(file.path(data.folder, "info_json.rds"))){
#  extra_info <- readRDS(file.path(data.folder, "info_json.rds"))
#}else{
#  source(file.path(code.folder,"code_2_features_json_file.R"))
#  saveRDS(extra_info, file = file.path(data.folder, "info_json.Rds"))
#}
#data.full <- merge(data.full, extra_info, by = "media_id", all.x = TRUE)
### add time-related variables
source(file.path(code.folder, "code_2_features_time_related.R"))
rm(list = "temp")
########## 4. DATA PARTITIONING
# Extract last 10 observations for each user, if possible
# Will move rare users completely to the test set
#data.full[data.full[dataset == "train", list(index = head(.I, 10)), by = user_id]$index, dataset := "test"]
# testing set: last 3 first_flow songs per user (if available)
# training set: all remaining songs
data.full[first_flow == 1 & dataset == "train", dataset := "test_candidate"] # here: 169,498 cases in testing
data.full[data.full[dataset == "test_candidate", list(index = tail(.I, 3)), by = user_id]$index, dataset := "test"] # here: 39,821 cases in testing
data.full[dataset == "test_candidate", dataset := "train"]
table(data.full$dataset)
########## 5. CREATING FEATURES ON PARTITIONED DATA
### Compute total plays and skips as features
source(file.path(code.folder, "code_2_features_total_plays.R"))
rm(list = "data.train")
### Compute naive skip ratios as features
source(file.path(code.folder, "code_2_features_naive_ratios.R"))
rm(list = "data.train")
rm(list = ls())
