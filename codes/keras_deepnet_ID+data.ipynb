{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# libraries\n",
    "#!pip install rpy2\n",
    "#!pip install pandas\n",
    "#!pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the necessary packages, especially the keras layers using hte layer names directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# libraries\n",
    "#import rpy2.robjects as robjects\n",
    "import pandas as pd\n",
    "import keras\n",
    "import numpy as np\n",
    "from keras.layers import Input, Dense, Embedding, concatenate, Flatten, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "#path = \"/Users/Kozodoi/Documents/Competitions/DSG_2017/\"\n",
    "path = \"/Users/maj/Dropbox/DSG17/DSG_2017/\"\n",
    "data   = pd.read_csv(path + \"data/data_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['user_id', 'media_id', 'artist_id', 'genre_id', 'album_id',\n",
      "       'context_type', 'media_duration', 'listen_type', 'user_gender',\n",
      "       'user_age', 'is_listened', 'sample_id', 'dataset', 'time_lag',\n",
      "       'session_id', 'song_session_position', 'first_flow', 'time_diff',\n",
      "       'hours', 'genre_plays', 'genre_skips', 'artist_plays', 'artist_skips',\n",
      "       'album_plays', 'album_skips', 'song_plays', 'song_skips',\n",
      "       'user_ratio_flow', 'user_ratio_full', 'genre_ratio', 'artist_ratio',\n",
      "       'song_ratio', 'platform_name1', 'platform_name2', 'platform_family1',\n",
      "       'platform_family2'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the combined data into the training, test and unknown set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tr = data[data.dataset == 'train']\n",
    "ts = data[data.dataset == 'test']\n",
    "unknown = data[data.dataset == 'unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2275140, 36)\n",
      "(193239, 36)\n",
      "Index(['user_id', 'media_id', 'artist_id', 'genre_id', 'album_id',\n",
      "       'context_type', 'media_duration', 'listen_type', 'user_gender',\n",
      "       'user_age', 'is_listened', 'sample_id', 'dataset', 'time_lag',\n",
      "       'session_id', 'song_session_position', 'first_flow', 'time_diff',\n",
      "       'hours', 'genre_plays', 'genre_skips', 'artist_plays', 'artist_skips',\n",
      "       'album_plays', 'album_skips', 'song_plays', 'song_skips',\n",
      "       'user_ratio_flow', 'user_ratio_full', 'genre_ratio', 'artist_ratio',\n",
      "       'song_ratio', 'platform_name1', 'platform_name2', 'platform_family1',\n",
      "       'platform_family2'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# keep only Flow songs in the data\n",
    "# this proves to predict better, but some information is lost\n",
    "tr = tr.query(\"listen_type == 1\")\n",
    "print(tr.shape)\n",
    "print(ts.shape)\n",
    "print(tr.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13451\n",
      "13451\n",
      "16826\n",
      "16826\n",
      "1334\n",
      "1334\n",
      "55975\n",
      "55975\n",
      "34\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "print(data.user_id.nunique())\n",
    "print(tr.user_id.nunique())\n",
    "print(data.artist_id.nunique())\n",
    "print(tr.artist_id.nunique())\n",
    "print(data.genre_id.nunique())\n",
    "print(tr.genre_id.nunique())\n",
    "print(data.media_id.nunique())\n",
    "print(tr.media_id.nunique())\n",
    "print(data.context_type.nunique())\n",
    "print(tr.context_type.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum value should be equal to the number of unique values - 1 (Python indexing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55974"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(tr.media_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the full feature set, create a subset of the data with the features that can be passed to the model directly. That is everything excluding the IDs and large factor variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the data input matrix that can be passed to the keras model\n",
    "# i.e. only numeric and without IDs and target variable\n",
    "dropVars = ['dataset','user_id', 'artist_id', 'media_id', \"genre_id\", \"album_id\", \"session_id\", \"is_listened\", \"sample_id\"]\n",
    "tr_data = tr[[column for column in tr.columns if column not in dropVars]].as_matrix()\n",
    "ts_data = ts[[column for column in tr.columns if column not in dropVars]].as_matrix()\n",
    "unknown_data = unknown[[column for column in tr.columns if column not in dropVars]].as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of the keras model is as follows:\n",
    "- Input layers ot specify the size of the data that goes in the model\n",
    "- (Embedding layer, i.e. lookup table layer that assigns 50 values to each level. These values are then trained to somehow capture the essence of this level.)\n",
    "- Dense layer, i.e. fully connected neural net layers\n",
    "- Output layer, i.e. Dense layer with only one result and sigmoid activation (for a result between 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create an input layer with one row of IDs\n",
    "user_in   = Input(shape = (1,), dtype='int64', name = \"user_in\")\n",
    "song_in   = Input(shape = (1,), dtype='int64', name = \"song_in\")\n",
    "artist_in = Input(shape = (1,), dtype='int64', name = \"artist_in\")\n",
    "genre_in  = Input(shape = (1,), dtype='int64', name = \"genre_in\")\n",
    "context_in = Input(shape = (1,), dtype='int64',   name = \"context_in\")\n",
    "\n",
    "# Create an embedding assigning k latent factors to each ID\n",
    "# These will be optimized\n",
    "# A regulariztaion is added to avoid very large weights\n",
    "n_users   = tr.user_id.nunique()\n",
    "n_songs   = tr.media_id.nunique()\n",
    "n_artists = tr.artist_id.nunique()\n",
    "n_genres  = tr.genre_id.nunique()\n",
    "n_context = tr.context_type.nunique()\n",
    "\n",
    "# The layer needs the number of the input levels and the number of values for each level\n",
    "user_embedding = Embedding(n_users,   50, input_length=1, embeddings_regularizer=l2(1e-5))(user_in)\n",
    "song_embedding = Embedding(n_songs,   50, input_length=1, embeddings_regularizer=l2(1e-5))(song_in)\n",
    "artist_embedding = Embedding(n_artists, 20, input_length=1, embeddings_regularizer=l2(1e-5))(artist_in)\n",
    "genre_embedding = Embedding(n_genres,  20, input_length=1, embeddings_regularizer=l2(1e-5))(genre_in)\n",
    "context_embedding = Embedding(n_context, 50, input_length=1, embeddings_regularizer=l2(1e-5))(context_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data input doesn't need any embedding and can directly be passed to a Dense layer\n",
    "data_in = Input(shape = (tr_data.shape[1],), name = \"data_in\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Specify what to do with the layers\n",
    "# We want to multiply them into a 'rating' matrix\n",
    "#\n",
    "embedding_input = concatenate([user_embedding,  song_embedding, artist_embedding, genre_embedding])\n",
    "embedding_input = Flatten()(embedding_input)\n",
    "embedding_dense = Dense(128, activation = \"relu\")(embedding_input)\n",
    "embedding_dense = BatchNormalization()(embedding_dense)\n",
    "#embedding_dense = Flatten()(embedding_dense)\n",
    "\n",
    "#data_input = Flatten()(data_in)\n",
    "data_dense = Dense(128, activation = \"relu\")(data_in)\n",
    "data_dense = BatchNormalization()(data_dense)\n",
    "# Make into a vector, i.e. drop 2D structure\n",
    "# The 2D structure is important for e.g. CNN filters,\n",
    "# but not necessary in a dense layer, I think\n",
    "x = concatenate([embedding_dense, data_dense])\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "# Correct the standard devitation calculated from a batch\n",
    "# to better fit the 'true' sd\n",
    "x = BatchNormalization()(x)\n",
    "# \"Drop\" each node at a training stage with a certain probability \n",
    "# then reinsert it after the training run\n",
    "# Avoids overfitting and increases speed\n",
    "x = Dropout(0.5)(x)\n",
    "#x = Dropout(0.5)(Dense(128, activation='relu')(x))\n",
    "#x = BatchNormalization()(x)\n",
    "#x = Dense(64, activation='relu')(x) \n",
    "#x = Dropout(0.5)(x)\n",
    "output = Dense(1, activation = \"sigmoid\")(x)\n",
    "#x = merge([x, ub], mode = 'sum')\n",
    "#x = merge([x, sb], mode = 'sum') # Can this be included in the line above?\n",
    "\n",
    "# Then we specify the model that we want to use\n",
    "#\n",
    "model = Model([user_in, song_in, artist_in, genre_in, data_in], output) # \n",
    "model.compile(Adam(0.001), loss=\"binary_crossentropy\", metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2275140 samples, validate on 193239 samples\n",
      "Epoch 1/1\n",
      " 547536/2275140 [======>.......................] - ETA: 143s - loss: 0.8496 - acc: 0.5551"
     ]
    }
   ],
   "source": [
    "# run the estimations\n",
    "#\n",
    "#\n",
    "model.fit([tr.user_id, tr.media_id, tr.artist_id, tr.genre_id, tr_data], tr.is_listened,  #\n",
    "validation_data = ([ts.user_id, ts.media_id, ts.artist_id, ts.genre_id, ts_data], ts.is_listened),\n",
    "batch_size = 22814, epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict on unlabelled set\n",
    "pred = pd.DataFrame()\n",
    "pred[\"sample_id\"] = unknown.sample_id\n",
    "pred[\"is_listened\"] = model.predict([unknown.userIdx, unknown.songIdx, unknown.artistIdx, unknown.genreIdx])\n",
    "pred.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# adding naive submission\n",
    "naive = pd.read_csv(path + \"submissions/naive_ratio_user.csv\")\n",
    "pred_mean = pred\n",
    "pred_mean[\"is_listened\"] = (pred[\"is_listened\"] + naive[\"is_listened\"])/2\n",
    "pred_mean.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# saving submissions\n",
    "pred.to_csv(path + \"submissions/deep_128_64_flow.csv\", index = False)\n",
    "pred.to_csv(path + \"submissions/deep_128_64_flow_plus_ratio_user.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [deeplearning]",
   "language": "python",
   "name": "Python [deeplearning]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
