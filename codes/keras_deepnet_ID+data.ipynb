{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# libraries\n",
    "#!pip install rpy2\n",
    "#!pip install pandas\n",
    "#!pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the necessary packages, especially the keras layers using hte layer names directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "# libraries\n",
    "#import rpy2.robjects as robjects\n",
    "\n",
    "# Data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# ML\n",
    "from sklearn import preprocessing, metrics\n",
    "# Keras\n",
    "import keras\n",
    "from keras.layers import Input, Dense, Embedding, concatenate, Flatten, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "#path = \"/Users/Kozodoi/Documents/Competitions/DSG_2017/\"\n",
    "path = \"/Users/hauptjoh/Dropbox/DSG17/DSG_2017/\"\n",
    "data   = pd.read_csv(path + \"data/data_full.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['user_id', 'media_id', 'artist_id', 'genre_id', 'album_id',\n",
      "       'context_type', 'media_duration', 'listen_type', 'user_gender',\n",
      "       'user_age', 'is_listened', 'sample_id', 'dataset', 'time_lag',\n",
      "       'session_id', 'song_session_position', 'first_flow', 'time_diff',\n",
      "       'release_year', 'genre_plays', 'genre_skips', 'artist_plays',\n",
      "       'artist_skips', 'album_plays', 'album_skips', 'song_plays',\n",
      "       'song_skips', 'user_ratio_flow', 'user_ratio_full', 'genre_ratio',\n",
      "       'artist_ratio', 'song_ratio', 'platform_name0', 'platform_name1',\n",
      "       'platform_name2', 'platform_family1', 'platform_family2',\n",
      "       'hour_of_day2', 'hour_of_day3', 'hour_of_day4', 'hour_of_day5',\n",
      "       'hour_of_day6', 'hour_of_day7', 'hour_of_day8', 'weekdayMon',\n",
      "       'weekdaySat', 'weekdaySun', 'weekdayThu', 'weekdayTue', 'weekdayWed'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the combined data into the training, test and unknown set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = data[data.dataset == 'train']\n",
    "# keep only Flow songs in the data\n",
    "# this proves to predict better, but some information is lost\n",
    "tr = tr.query(\"listen_type == 1\")\n",
    "\n",
    "ts = data[data.dataset == 'test']\n",
    "known = data.query(\"dataset == 'train' or dataset == 'test'\")\n",
    "unknown = data[data.dataset == 'unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2179592, 50)\n",
      "(140019, 50)\n",
      "Index(['user_id', 'media_id', 'artist_id', 'genre_id', 'album_id',\n",
      "       'context_type', 'media_duration', 'listen_type', 'user_gender',\n",
      "       'user_age', 'is_listened', 'sample_id', 'dataset', 'time_lag',\n",
      "       'session_id', 'song_session_position', 'first_flow', 'time_diff',\n",
      "       'release_year', 'genre_plays', 'genre_skips', 'artist_plays',\n",
      "       'artist_skips', 'album_plays', 'album_skips', 'song_plays',\n",
      "       'song_skips', 'user_ratio_flow', 'user_ratio_full', 'genre_ratio',\n",
      "       'artist_ratio', 'song_ratio', 'platform_name0', 'platform_name1',\n",
      "       'platform_name2', 'platform_family1', 'platform_family2',\n",
      "       'hour_of_day2', 'hour_of_day3', 'hour_of_day4', 'hour_of_day5',\n",
      "       'hour_of_day6', 'hour_of_day7', 'hour_of_day8', 'weekdayMon',\n",
      "       'weekdaySat', 'weekdaySun', 'weekdayThu', 'weekdayTue', 'weekdayWed'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(tr.shape)\n",
    "print(ts.shape)\n",
    "print(tr.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13811\n",
      "13811\n",
      "20190\n",
      "20190\n",
      "1493\n",
      "1493\n",
      "76267\n",
      "76267\n",
      "34\n",
      "34\n"
     ]
    }
   ],
   "source": [
    "# Check if IDs are valid, should not be smaller for the (2nd row) train set\n",
    "print(data.user_id.nunique())\n",
    "print(tr.user_id.nunique())\n",
    "print(data.artist_id.nunique())\n",
    "print(tr.artist_id.nunique())\n",
    "print(data.genre_id.nunique())\n",
    "print(tr.genre_id.nunique())\n",
    "print(data.media_id.nunique())\n",
    "print(tr.media_id.nunique())\n",
    "print(data.context_type.nunique())\n",
    "print(tr.context_type.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum value should be equal to the number of unique values - 1 (Python indexing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76266"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(tr.media_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the full feature set, create a subset of the data with the features that can be passed to the model directly. That is everything excluding the IDs and large factor variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the data input matrix that can be passed to the keras model\n",
    "# i.e. only numeric and without IDs and target variable\n",
    "dropVars = ['dataset','user_id', 'artist_id', 'media_id', \"genre_id\", \"album_id\", \"session_id\", \"is_listened\", \"sample_id\"]\n",
    "tr_data = tr[[column for column in tr.columns if column not in dropVars]].as_matrix()\n",
    "ts_data = ts[[column for column in ts.columns if column not in dropVars]].as_matrix()\n",
    "known_data = known[[column for column in known.columns if column not in dropVars]].as_matrix()\n",
    "unknown_data = unknown[[column for column in unknown.columns if column not in dropVars]].as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numeric data should be normalized to bring it to a similar range. This helps the network by making sure that the weights can also be in a similar, small range and do not need to take into account the different scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "# Create an object to transform the data to fit minmax processor\n",
    "scaler = preprocessing.StandardScaler().fit(tr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the normalizer on the dataframe\n",
    "tr_data = scaler.transform(tr_data)\n",
    "ts_data = scaler.transform(ts_data)\n",
    "known_data = scaler.transform(known_data)\n",
    "unknown_data = scaler.transform(unknown_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The structure of the keras model is as follows:\n",
    "- Input layers ot specify the size of the data that goes in the model\n",
    "- (Embedding layer, i.e. lookup table layer that assigns 50 values to each level. These values are then trained to somehow capture the essence of this level.)\n",
    "- Dense layer, i.e. fully connected neural net layers\n",
    "- Output layer, i.e. Dense layer with only one result and sigmoid activation (for a result between 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an input layer with one row of IDs\n",
    "user_in   = Input(shape = (1,), dtype='int64', name = \"user_in\")\n",
    "song_in   = Input(shape = (1,), dtype='int64', name = \"song_in\")\n",
    "artist_in = Input(shape = (1,), dtype='int64', name = \"artist_in\")\n",
    "genre_in  = Input(shape = (1,), dtype='int64', name = \"genre_in\")\n",
    "context_in = Input(shape = (1,), dtype='int64',   name = \"context_in\")\n",
    "\n",
    "# Create an embedding assigning k latent factors to each ID\n",
    "# These will be optimized\n",
    "# A regulariztaion is added to avoid very large weights\n",
    "n_users   = tr.user_id.nunique()\n",
    "n_songs   = tr.media_id.nunique()\n",
    "n_artists = tr.artist_id.nunique()\n",
    "n_genres  = tr.genre_id.nunique()\n",
    "n_context = tr.context_type.nunique()\n",
    "\n",
    "# The layer needs the number of the input levels and the number of values for each level\n",
    "user_embedding = Embedding(n_users,   50, input_length=1, embeddings_regularizer=l2(1e-5))(user_in)\n",
    "song_embedding = Embedding(n_songs,   50, input_length=1, embeddings_regularizer=l2(1e-5))(song_in)\n",
    "artist_embedding = Embedding(n_artists, 50, input_length=1, embeddings_regularizer=l2(1e-5))(artist_in)\n",
    "genre_embedding = Embedding(n_genres,  50, input_length=1, embeddings_regularizer=l2(1e-5))(genre_in)\n",
    "context_embedding = Embedding(n_context, 50, input_length=1, embeddings_regularizer=l2(1e-5))(context_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data input doesn't need any embedding and can directly be passed to a Dense layer\n",
    "data_in = Input(shape = (tr_data.shape[1],), name = \"data_in\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify what to do with the layers\n",
    "embedding_input = concatenate([user_embedding,  song_embedding, artist_embedding, genre_embedding])\n",
    "embedding_input = Flatten()(embedding_input)\n",
    "embedding_dense = Dense(256, activation = \"relu\")(embedding_input)\n",
    "embedding_dense = BatchNormalization()(embedding_dense)\n",
    "#embedding_dense = Flatten()(embedding_dense)\n",
    "\n",
    "#data_input = Flatten()(data_in)\n",
    "data_dense = Dense(256, activation = \"relu\")(data_in)\n",
    "data_dense = BatchNormalization()(data_dense)\n",
    "# Make into a vector, i.e. drop 2D structure\n",
    "# The 2D structure is important for e.g. CNN filters,\n",
    "# but not necessary in a dense layer, I think\n",
    "x = concatenate([embedding_dense, data_dense])\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "# Correct the standard devitation calculated from a batch\n",
    "# to better fit the 'true' sd\n",
    "x = BatchNormalization()(x)\n",
    "# \"Drop\" each node at a training stage with a certain probability \n",
    "# then reinsert it after the training run\n",
    "# Avoids overfitting and increases speed\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(128, activation='relu')(x) \n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(1, activation = \"sigmoid\")(x)\n",
    "#x = merge([x, ub], mode = 'sum')\n",
    "#x = merge([x, sb], mode = 'sum') # Can this be included in the line above?\n",
    "\n",
    "# Then we specify the model that we want to use\n",
    "#\n",
    "model = Model([user_in, song_in, artist_in, genre_in, data_in], output) # \n",
    "model.compile(optimizer=\"Adagrad\", loss=\"binary_crossentropy\", metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2179592 samples, validate on 140019 samples\n",
      "Epoch 1/10\n",
      " 365024/2179592 [====>.........................] - ETA: 341s - loss: 0.4455 - acc: 0.7868"
     ]
    }
   ],
   "source": [
    "# run the estimations\n",
    "model.fit([tr.user_id, tr.media_id, tr.artist_id, tr.genre_id, tr_data], tr.is_listened,  #\n",
    "validation_data = ([ts.user_id, ts.media_id, ts.artist_id, ts.genre_id, ts_data], ts.is_listened),\n",
    "batch_size = 22814, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69867476601425826"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict validation data\n",
    "pred = pd.DataFrame()\n",
    "pred[\"user_id\"]  = ts.user_id\n",
    "pred[\"media_id\"] = ts.media_id\n",
    "pred[\"is_listened\"] = model.predict([ts.user_id, ts.media_id, ts.artist_id, ts.genre_id, ts_data])\n",
    "pred.to_csv(path + \"data/keras_ts_emd+data_epoch10+5.csv\", index = False)\n",
    "pred.head(5)\n",
    "metrics.roc_auc_score(ts.is_listened, pred.is_listened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7558834/7558834 [==============================] - 1299s - loss: 0.5054 - acc: 0.7604  \n",
      "Epoch 2/10\n",
      "7558834/7558834 [==============================] - 1287s - loss: 0.4873 - acc: 0.7696  \n",
      "Epoch 3/10\n",
      "7558834/7558834 [==============================] - 1287s - loss: 0.4825 - acc: 0.7722  \n",
      "Epoch 4/10\n",
      "7558834/7558834 [==============================] - 1292s - loss: 0.4793 - acc: 0.7736  \n",
      "Epoch 5/10\n",
      "7558834/7558834 [==============================] - 1286s - loss: 0.4761 - acc: 0.7755  \n",
      "Epoch 6/10\n",
      "7558834/7558834 [==============================] - 1289s - loss: 0.4736 - acc: 0.7766  \n",
      "Epoch 7/10\n",
      "7558834/7558834 [==============================] - 1285s - loss: 0.4709 - acc: 0.7779  \n",
      "Epoch 8/10\n",
      "7558834/7558834 [==============================] - 1287s - loss: 0.4691 - acc: 0.7789  \n",
      "Epoch 9/10\n",
      "7558834/7558834 [==============================] - 1287s - loss: 0.4674 - acc: 0.7797  \n",
      "Epoch 10/10\n",
      "7558834/7558834 [==============================] - 1292s - loss: 0.4659 - acc: 0.7806  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a4bc4160>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the estimations on full known data\n",
    "model.fit([known.user_id, known.media_id, known.artist_id, known.genre_id, known_data], known.is_listened,\n",
    "batch_size = int(known.shape[0]/100), epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>is_listened</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6317</th>\n",
       "      <td>14561</td>\n",
       "      <td>0.968824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12558</th>\n",
       "      <td>6026</td>\n",
       "      <td>0.848141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18770</th>\n",
       "      <td>9627</td>\n",
       "      <td>0.989767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24352</th>\n",
       "      <td>6064</td>\n",
       "      <td>0.838861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29779</th>\n",
       "      <td>8065</td>\n",
       "      <td>0.172180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sample_id  is_listened\n",
       "6317       14561     0.968824\n",
       "12558       6026     0.848141\n",
       "18770       9627     0.989767\n",
       "24352       6064     0.838861\n",
       "29779       8065     0.172180"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict unknown data\n",
    "pred = pd.DataFrame()\n",
    "pred[\"sample_id\"] = unknown.sample_id.astype('int')\n",
    "pred[\"is_listened\"] = model.predict([unknown.user_id, unknown.media_id, unknown.artist_id, unknown.genre_id, unknown_data])\n",
    "pred.to_csv(path + \"submissions/keras_uk_emb+data_epoch10+5_formatting.csv\", index = False)\n",
    "pred.head(5)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
