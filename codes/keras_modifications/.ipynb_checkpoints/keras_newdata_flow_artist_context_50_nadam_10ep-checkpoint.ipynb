{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn\n",
      "  Downloading imblearn-0.0-py2.py3-none-any.whl\n",
      "Collecting imbalanced-learn (from imblearn)\n",
      "  Downloading imbalanced-learn-0.2.1.tar.gz (90kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 660kB/s \n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.17.0 in /anaconda/lib/python3.5/site-packages (from imbalanced-learn->imblearn)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /anaconda/lib/python3.5/site-packages (from imbalanced-learn->imblearn)\n",
      "Requirement already satisfied: scikit-learn>=0.17.1 in /anaconda/lib/python3.5/site-packages (from imbalanced-learn->imblearn)\n",
      "Building wheels for collected packages: imbalanced-learn\n",
      "  Running setup.py bdist_wheel for imbalanced-learn ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
      "\u001b[?25h  Stored in directory: /Users/Kozodoi/Library/Caches/pip/wheels/b8/20/bd/0b775f7e5d413ac72562b1a5126598bcb6e0eae10da659be9f\n",
      "Successfully built imbalanced-learn\n",
      "Installing collected packages: imbalanced-learn, imblearn\n",
      "Successfully installed imbalanced-learn-0.2.1 imblearn-0.0\n"
     ]
    }
   ],
   "source": [
    "# libraries\n",
    "#!pip install rpy2\n",
    "#!pip install pandas\n",
    "#!pip install keras\n",
    "#!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# libraries\n",
    "import rpy2.robjects as robjects\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.layers import Input, Dense, Embedding, concatenate, Flatten, Dropout, Reshape\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2339529, 59)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "path = \"/Users/Kozodoi/Documents/Competitions/DSG_2017/\"\n",
    "data = pd.read_csv(path + \"data/data_flow.csv\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add observation index\n",
    "data[\"row_index\"] = data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (2279790, 60)\n",
      "test: (39821, 60)\n",
      "known: (2319611, 60)\n",
      "unknown: (19918, 60)\n"
     ]
    }
   ],
   "source": [
    "# data partitioning\n",
    "tr = data.query(\"dataset == 'train'\")\n",
    "ts = data.query(\"dataset == 'test'\")\n",
    "kn = data.query(\"dataset != 'unknown'\")\n",
    "un = data.query(\"dataset == 'unknown'\")\n",
    "\n",
    "# print data sizes\n",
    "print(\"train: \"   + str(tr.shape))\n",
    "print(\"test: \"    + str(ts.shape))\n",
    "print(\"known: \"   + str(kn.shape))\n",
    "print(\"unknown: \" + str(un.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['user_id', 'context_type', 'media_id', 'artist_id', 'genre_id',\n",
      "       'media_duration', 'listen_type', 'user_gender', 'user_age',\n",
      "       'is_listened', 'sample_id', 'dataset', 'session_id',\n",
      "       'song_session_position', 'first_flow', 'time_diff_release_listen',\n",
      "       'release_year', 'is_listened_lag1', 'is_listened_lag2',\n",
      "       'user_skip_ratio_last3', 'user_skip_ratio_last5',\n",
      "       'user_skip_ratio_last10', 'genre_equal_last_song',\n",
      "       'artist_equal_last_song', 'album_equal_last_song', 'genre_plays',\n",
      "       'genre_skips', 'artist_plays', 'artist_skips', 'album_plays',\n",
      "       'album_skips', 'song_plays', 'song_skips', 'user_ratio_flow',\n",
      "       'user_ratio_full', 'genre_ratio', 'artist_ratio', 'song_ratio',\n",
      "       'context_ratio', 'user_genre_ratio', 'user_artist_ratio',\n",
      "       'user_song_ratio', 'user_context_ratio', 'platform_name',\n",
      "       'platform_family', 'hour_of_day1', 'hour_of_day2', 'hour_of_day3',\n",
      "       'hour_of_day4', 'hour_of_day5', 'hour_of_day6', 'hour_of_day7',\n",
      "       'hour_of_day8', 'weekdayMon', 'weekdaySat', 'weekdaySun', 'weekdayThu',\n",
      "       'weekdayTue', 'weekdayWed', 'row_index'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# List numeric features used as predictors\n",
    "print(data.columns)\n",
    "numVars = [\"user_ratio_flow\", \"user_ratio_full\", \"listen_type\", \"first_flow\",\n",
    "           \"song_plays\", \"artist_plays\", \"platform_name1\", \"platform_name2\",\n",
    "           \"song_skips\", \"artist_skips\", \"song_session_position\", \"time_diff\"] \n",
    "\n",
    "# Create the data input matrix that can be passed to the keras model\n",
    "tr_data = tr[[column for column in tr.columns if column in numVars]].as_matrix()\n",
    "ts_data = ts[[column for column in ts.columns if column in numVars]].as_matrix()\n",
    "kn_data = kn[[column for column in kn.columns if column in numVars]].as_matrix()\n",
    "un_data = un[[column for column in un.columns if column in numVars]].as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. INITIALIZING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an input layer with embeddings\n",
    "user_in    = Input(shape = (1,), dtype = 'int64',   name = \"user_in\")\n",
    "song_in    = Input(shape = (1,), dtype = 'int64',   name = \"song_in\")\n",
    "artist_in  = Input(shape = (1,), dtype = 'int64',   name = \"artist_in\")\n",
    "context_in = Input(shape = (1,), dtype = 'int64',   name = \"context_in\")\n",
    "\n",
    "# Create an input layer with numeric features\n",
    "data_in = Input(shape = (tr_data.shape[1],), name = \"data_in\")\n",
    "\n",
    "# Counting number of unique ID values\n",
    "n_users   = tr.user_id.nunique()\n",
    "n_songs   = tr.media_id.nunique()\n",
    "n_artists = tr.artist_id.nunique()\n",
    "n_context = tr.context_type.nunique()\n",
    "\n",
    "# Create an embedding assigning k latent factors to each ID\n",
    "u = Embedding(n_users,   50, input_length = 1, embeddings_regularizer = l2(1e-5))(user_in)\n",
    "s = Embedding(n_songs,   50, input_length = 1, embeddings_regularizer = l2(1e-5))(song_in)\n",
    "a = Embedding(n_artists, 50, input_length = 1, embeddings_regularizer = l2(1e-5))(artist_in)\n",
    "c = Embedding(n_context, 50, input_length = 1, embeddings_regularizer = l2(1e-5))(context_in)\n",
    "\n",
    "\n",
    "# Layer with embeddings\n",
    "embedding_input = concatenate([u, a, c])\n",
    "embedding_input = Flatten()(embedding_input)\n",
    "embedding_dense = Dense(128, activation = \"relu\")(embedding_input)\n",
    "embedding_dense = BatchNormalization()(embedding_dense)\n",
    "\n",
    "# Layer with numeric features\n",
    "data_dense = Dense(16, activation = \"relu\")(data_in)\n",
    "data_dense = BatchNormalization()(data_dense)\n",
    "\n",
    "# Constructing the further layers\n",
    "x = concatenate([embedding_dense, data_dense])\n",
    "x = Dropout(0.5)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(64, activation='relu')(x) \n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(1, activation = \"sigmoid\")(x)\n",
    "\n",
    "# Specify the model that we want to use\n",
    "model = Model([user_in, artist_in, context_in], output)\n",
    "model.compile(optimizer = \"Nadam\", loss = \"binary_crossentropy\", metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. FIRST STAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2279790 samples, validate on 39821 samples\n",
      "Epoch 1/10\n",
      "2279790/2279790 [==============================] - 111s - loss: 0.5226 - acc: 0.7468 - val_loss: 0.6593 - val_acc: 0.6168\n",
      "Epoch 2/10\n",
      "2279790/2279790 [==============================] - 117s - loss: 0.4914 - acc: 0.7668 - val_loss: 0.6558 - val_acc: 0.6681\n",
      "Epoch 3/10\n",
      "2279790/2279790 [==============================] - 124s - loss: 0.4862 - acc: 0.7686 - val_loss: 0.6207 - val_acc: 0.6661\n",
      "Epoch 4/10\n",
      "2279790/2279790 [==============================] - 152s - loss: 0.4825 - acc: 0.7702 - val_loss: 0.6124 - val_acc: 0.6748\n",
      "Epoch 5/10\n",
      "2279790/2279790 [==============================] - 161s - loss: 0.4801 - acc: 0.7711 - val_loss: 0.6089 - val_acc: 0.6750\n",
      "Epoch 6/10\n",
      "1755369/2279790 [======================>.......] - ETA: 39s - loss: 0.4784 - acc: 0.7714"
     ]
    }
   ],
   "source": [
    "# run the estimations on training data\n",
    "model.fit([tr.user_id, tr.artist_id, tr.context_type, tr.data_in], tr.is_listened, \n",
    "validation_data = ([ts.user_id, ts.artist_id, ts.context_type, ts.data_in], ts.is_listened),\n",
    "batch_size = int(len(tr)/100), epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict validation data\n",
    "pred = pd.DataFrame()\n",
    "pred[\"row_index\"] = ts.row_index\n",
    "pred[\"is_listened\"] = model.predict([ts.user_id, ts.artist_id, ts.context_type, ts.data_in])\n",
    "pred.to_csv(path + \"pred_valid/keras_newdata_flow_artist_context_50_nadam_10ep.csv\", index = False)\n",
    "pred.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# computing AUC\n",
    "metrics.roc_auc_score(ts.is_listened, pred.is_listened)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. SECOND STAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2319611/2319611 [==============================] - 146s - loss: 0.4735 - acc: 0.7724   \n",
      "Epoch 7/10\n",
      "2319611/2319611 [==============================] - 113s - loss: 0.4730 - acc: 0.7726   \n",
      "Epoch 8/10\n",
      "2319611/2319611 [==============================] - 105s - loss: 0.4721 - acc: 0.7727   \n",
      "Epoch 9/10\n",
      "2319611/2319611 [==============================] - 106s - loss: 0.4721 - acc: 0.7727   \n",
      "Epoch 10/10\n",
      "2319611/2319611 [==============================] - 105s - loss: 0.4714 - acc: 0.7730   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x120daa668>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the estimations on full known data\n",
    "model.fit([kn.user_id, kn.artist_id, kn.context_type, kn.data_in], kn.is_listened,\n",
    "batch_size = int(kn.shape[0]/100), epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>is_listened</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2332218</th>\n",
       "      <td>0</td>\n",
       "      <td>0.983333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2182792</th>\n",
       "      <td>1</td>\n",
       "      <td>0.488397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2092769</th>\n",
       "      <td>2</td>\n",
       "      <td>0.747803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1806368</th>\n",
       "      <td>3</td>\n",
       "      <td>0.454087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018166</th>\n",
       "      <td>4</td>\n",
       "      <td>0.872301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sample_id  is_listened\n",
       "2332218          0     0.983333\n",
       "2182792          1     0.488397\n",
       "2092769          2     0.747803\n",
       "1806368          3     0.454087\n",
       "2018166          4     0.872301"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict unknown data\n",
    "pred = pd.DataFrame()\n",
    "pred[\"sample_id\"] = un.sample_id.astype(int)\n",
    "pred[\"is_listened\"] = model.predict([un.user_id, un.artist_id, un.context_type, un.data_in])\n",
    "pred = pred.sort_values(\"sample_id\")\n",
    "pred.to_csv(path + \"pred_unknown/keras_newdata_flow_artist_context_50_nadam_10ep.csv\", index = False)\n",
    "pred.head(5)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
