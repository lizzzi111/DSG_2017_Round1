{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# libraries\n",
    "#!pip install rpy2\n",
    "#!pip install pandas\n",
    "#!pip install keras\n",
    "#!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# libraries\n",
    "import rpy2.robjects as robjects\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.layers import Input, Dense, Embedding, concatenate, Flatten, Dropout, Reshape\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2339529, 35)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "path = \"/Users/Kozodoi/Documents/Competitions/DSG_2017/\"\n",
    "data = pd.read_csv(path + \"data/data_flow.csv\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add observation index\n",
    "data[\"row_index\"] = data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: (2279790, 36)\n",
      "test: (39821, 36)\n",
      "known: (2319611, 36)\n",
      "unknown: (19918, 36)\n"
     ]
    }
   ],
   "source": [
    "# data partitioning\n",
    "tr = data.query(\"dataset == 'train'\")\n",
    "ts = data.query(\"dataset == 'test'\")\n",
    "kn = data.query(\"dataset != 'unknown'\")\n",
    "un = data.query(\"dataset == 'unknown'\")\n",
    "\n",
    "# print data sizes\n",
    "print(\"train: \"   + str(tr.shape))\n",
    "print(\"test: \"    + str(ts.shape))\n",
    "print(\"known: \"   + str(kn.shape))\n",
    "print(\"unknown: \" + str(un.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['user_id', 'media_id', 'artist_id', 'genre_id', 'context_type',\n",
      "       'media_duration', 'listen_type', 'user_gender', 'user_age',\n",
      "       'is_listened', 'sample_id', 'dataset', 'session_id',\n",
      "       'song_session_position', 'first_flow', 'time_diff', 'hours',\n",
      "       'is_listened_lag', 'genre_plays', 'genre_skips', 'artist_plays',\n",
      "       'artist_skips', 'album_plays', 'album_skips', 'song_plays',\n",
      "       'song_skips', 'user_ratio_flow', 'user_ratio_full', 'genre_ratio',\n",
      "       'artist_ratio', 'song_ratio', 'platform_name1', 'platform_name2',\n",
      "       'platform_family1', 'platform_family2', 'row_index'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# List numeric features used as predictors\n",
    "print(data.columns)\n",
    "numVars = [\"user_ratio_flow\", \"user_ratio_full\", \"listen_type\", \"first_flow\",\n",
    "           \"song_plays\", \"artist_plays\", \"platform_name1\", \"platform_name2\",\n",
    "           \"song_skips\", \"artist_skips\", \"song_session_position\", \"time_diff\"] \n",
    "\n",
    "# Create the data input matrix that can be passed to the keras model\n",
    "tr_data = tr[[column for column in tr.columns if column in numVars]].as_matrix()\n",
    "ts_data = ts[[column for column in ts.columns if column in numVars]].as_matrix()\n",
    "kn_data = kn[[column for column in kn.columns if column in numVars]].as_matrix()\n",
    "un_data = un[[column for column in un.columns if column in numVars]].as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MODELING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. INITIALIZING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an input layer with embeddings\n",
    "user_in    = Input(shape = (1,), dtype = 'int64',   name = \"user_in\")\n",
    "song_in    = Input(shape = (1,), dtype = 'int64',   name = \"song_in\")\n",
    "artist_in  = Input(shape = (1,), dtype = 'int64',   name = \"artist_in\")\n",
    "context_in = Input(shape = (1,), dtype = 'int64',   name = \"context_in\")\n",
    "\n",
    "# Create an input layer with numeric features\n",
    "data_in = Input(shape = (tr_data.shape[1],), name = \"data_in\")\n",
    "\n",
    "# Counting number of unique ID values\n",
    "n_users   = tr.user_id.nunique()\n",
    "n_songs   = tr.media_id.nunique()\n",
    "n_artists = tr.artist_id.nunique()\n",
    "n_context = tr.context_type.nunique()\n",
    "\n",
    "# Create an embedding assigning k latent factors to each ID\n",
    "u = Embedding(n_users,   50, input_length = 1, embeddings_regularizer = l2(1e-5))(user_in)\n",
    "s = Embedding(n_songs,   50, input_length = 1, embeddings_regularizer = l2(1e-5))(song_in)\n",
    "a = Embedding(n_artists, 50, input_length = 1, embeddings_regularizer = l2(1e-5))(artist_in)\n",
    "c = Embedding(n_context, 50, input_length = 1, embeddings_regularizer = l2(1e-5))(context_in)\n",
    "\n",
    "# Layer with embeddings\n",
    "embedding_input = concatenate([u, c])\n",
    "embedding_input = Flatten()(embedding_input)\n",
    "embedding_dense = Dense(128, activation = \"relu\")(embedding_input)\n",
    "embedding_dense = BatchNormalization()(embedding_dense)\n",
    "\n",
    "# Layer with numeric features\n",
    "data_dense = Dense(16, activation = \"relu\")(data_in)\n",
    "data_dense = BatchNormalization()(data_dense)\n",
    "\n",
    "# Constructing the further layers\n",
    "x = concatenate([embedding_dense, data_dense])\n",
    "x = Dropout(0.5)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(64, activation='relu')(x) \n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(1, activation = \"sigmoid\")(x)\n",
    "\n",
    "# Specify the model that we want to use\n",
    "model = Model([user_in, context_in, data_in], output)\n",
    "model.compile(optimizer = \"Adagrad\", loss = \"binary_crossentropy\", metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. FIRST STAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2279790 samples, validate on 39821 samples\n",
      "Epoch 1/10\n",
      "2279790/2279790 [==============================] - 92s - loss: 0.5289 - acc: 0.7384 - val_loss: 0.6705 - val_acc: 0.6085\n",
      "Epoch 2/10\n",
      "2279790/2279790 [==============================] - 73s - loss: 0.4877 - acc: 0.7652 - val_loss: 0.6584 - val_acc: 0.6084\n",
      "Epoch 3/10\n",
      "2279790/2279790 [==============================] - 53s - loss: 0.4823 - acc: 0.7680 - val_loss: 0.6359 - val_acc: 0.6403\n",
      "Epoch 4/10\n",
      "2279790/2279790 [==============================] - 68s - loss: 0.4788 - acc: 0.7695 - val_loss: 0.6229 - val_acc: 0.6568\n",
      "Epoch 5/10\n",
      "2279790/2279790 [==============================] - 76s - loss: 0.4766 - acc: 0.7705 - val_loss: 0.6086 - val_acc: 0.6719\n",
      "Epoch 6/10\n",
      "2279790/2279790 [==============================] - 77s - loss: 0.4747 - acc: 0.7714 - val_loss: 0.6037 - val_acc: 0.6789\n",
      "Epoch 7/10\n",
      "2279790/2279790 [==============================] - 75s - loss: 0.4735 - acc: 0.7719 - val_loss: 0.6089 - val_acc: 0.6794\n",
      "Epoch 8/10\n",
      "2279790/2279790 [==============================] - 76s - loss: 0.4721 - acc: 0.7726 - val_loss: 0.6182 - val_acc: 0.6747\n",
      "Epoch 9/10\n",
      "2279790/2279790 [==============================] - 74s - loss: 0.4711 - acc: 0.7731 - val_loss: 0.6222 - val_acc: 0.6786\n",
      "Epoch 10/10\n",
      "2279790/2279790 [==============================] - 77s - loss: 0.4702 - acc: 0.7734 - val_loss: 0.6309 - val_acc: 0.6800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12f18bc88>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the estimations on training data\n",
    "model.fit([tr.user_id, tr.context_type, tr_data], tr.is_listened, \n",
    "validation_data = ([ts.user_id, ts.context_type, ts_data], ts.is_listened),\n",
    "batch_size = int(len(tr)/100), epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_index</th>\n",
       "      <th>is_listened</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2167</th>\n",
       "      <td>2167</td>\n",
       "      <td>0.980167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2168</th>\n",
       "      <td>2168</td>\n",
       "      <td>0.989523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2240</th>\n",
       "      <td>2240</td>\n",
       "      <td>0.989574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4314</th>\n",
       "      <td>4314</td>\n",
       "      <td>0.985285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4321</th>\n",
       "      <td>4321</td>\n",
       "      <td>0.987488</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      row_index  is_listened\n",
       "2167       2167     0.980167\n",
       "2168       2168     0.989523\n",
       "2240       2240     0.989574\n",
       "4314       4314     0.985285\n",
       "4321       4321     0.987488"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict validation data\n",
    "pred = pd.DataFrame()\n",
    "pred[\"row_index\"] = ts.row_index\n",
    "pred[\"is_listened\"] = model.predict([ts.user_id, ts.context_type, ts_data])\n",
    "pred.to_csv(path + \"pred_valid/keras_newdata_flow_context50.csv\", index = False)\n",
    "pred.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7230561342740176"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# computing AUC\n",
    "metrics.roc_auc_score(ts.is_listened, pred.is_listened)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. SECOND STAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2319611/2319611 [==============================] - 86s - loss: 0.4724 - acc: 0.7722    \n",
      "Epoch 2/10\n",
      "2319611/2319611 [==============================] - 86s - loss: 0.4717 - acc: 0.7723    \n",
      "Epoch 3/10\n",
      "2319611/2319611 [==============================] - 110s - loss: 0.4716 - acc: 0.7726   \n",
      "Epoch 4/10\n",
      "2319611/2319611 [==============================] - 111s - loss: 0.4707 - acc: 0.7729   \n",
      "Epoch 5/10\n",
      "2319611/2319611 [==============================] - 109s - loss: 0.4701 - acc: 0.7731   \n",
      "Epoch 6/10\n",
      "2319611/2319611 [==============================] - 111s - loss: 0.4699 - acc: 0.7732   \n",
      "Epoch 7/10\n",
      "2319611/2319611 [==============================] - 108s - loss: 0.4696 - acc: 0.7735   \n",
      "Epoch 8/10\n",
      "2319611/2319611 [==============================] - 109s - loss: 0.4692 - acc: 0.7735   \n",
      "Epoch 9/10\n",
      "2319611/2319611 [==============================] - 112s - loss: 0.4691 - acc: 0.7735   \n",
      "Epoch 10/10\n",
      "2319611/2319611 [==============================] - 112s - loss: 0.4687 - acc: 0.7734   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x120daca20>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the estimations on full known data\n",
    "model.fit([kn.user_id, kn.context_type, kn_data], kn.is_listened,\n",
    "batch_size = int(kn.shape[0]/100), epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>is_listened</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2332218</th>\n",
       "      <td>0</td>\n",
       "      <td>0.979703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2182792</th>\n",
       "      <td>1</td>\n",
       "      <td>0.583291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2092769</th>\n",
       "      <td>2</td>\n",
       "      <td>0.687123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1806368</th>\n",
       "      <td>3</td>\n",
       "      <td>0.435899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018166</th>\n",
       "      <td>4</td>\n",
       "      <td>0.835211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sample_id  is_listened\n",
       "2332218          0     0.979703\n",
       "2182792          1     0.583291\n",
       "2092769          2     0.687123\n",
       "1806368          3     0.435899\n",
       "2018166          4     0.835211"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict unknown data\n",
    "pred = pd.DataFrame()\n",
    "pred[\"sample_id\"] = un.sample_id.astype(int)\n",
    "pred[\"is_listened\"] = model.predict([un.user_id, un.context_type, un_data])\n",
    "pred = pred.sort_values(\"sample_id\")\n",
    "pred.to_csv(path + \"pred_unknown/keras_newdata_flow_context50.csv\", index = False)\n",
    "pred.head(5)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
